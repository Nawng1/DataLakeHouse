
API link "https://environment.data.gov.uk/flood-monitoring/doc/reference"

The Environment Agency Real Time flood-monitoring API provides near real-time information about:
- Flood warnings and alerts.
- Flood areas to which warnings or alerts apply.
- Measurements of water levels and flows.
- Information on the monitoring stations providing those measurements.


data lakehouse architecture

1/ near real Time data API →(use kafka + python to load data to kafka) Kafka →use spark to load data to MinIO → 
2/ (read data from minio and transform data by Spark Bronze/Silver/Gold)  Spark containner + Delta Lake(file system)→ 
3/ Databricks SparkML+ MLflow (Huấn luyện, Đánh giá, Triển khai mo hinh)  
4/ Hive Metastore + Trino(read data from minio gold layer) + Superset(visualize data)



spark.conf.set("spark.hadoop.fs.s3a.endpoint", "http://192.168.1.12:9000")  # Replace with actual host IP
spark.conf.set("spark.hadoop.fs.s3a.access.key", "minio_access_key")  # From docker-compose
spark.conf.set("spark.hadoop.fs.s3a.secret.key", "minio_secret_key")  # From docker-compose
spark.conf.set("spark.hadoop.fs.s3a.path.style.access", "true")
spark.conf.set("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
spark.conf.set("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")


FROM python:3.9-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy the application code
COPY flood_data_pipeline.py .

RUN pip install --no-cache-dir kafka-python requests


check kafka topic

docker exec -it kafka kafka-topics --list --bootstrap-server kafka:29092

docker exec -it kafka kafka-console-consumer --bootstrap-server kafka:29092 --topic flood-warnings --from-beginning --max-messages 2

┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐
│   APIs/Kafka    │───▶│   Bronze Layer   │───▶│  Silver Layer   │
│   (Raw JSON)    │    │  (Raw Parquet)   │    │ (Delta Tables)  │
└─────────────────┘    └──────────────────┘    └─────────────────┘
                                                         │
                                                         ▼
┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐
│     Trino       │◀───│   Gold Layer     │◀───│   ML Pipeline   │
│   Analytics     │    │ (Delta Models)   │    │ (Delta Features)│
└─────────────────┘    └──────────────────┘    └─────────────────┘



after i load data to minio @extract , i have these data @bronze in minio in json format in brozen layer, now how can transfrom, data cleaning  it by spark cluster @docker-compose.yaml in silver layer, how can i transform it for meachine learning, spark ml and data vizualiazation with trino query